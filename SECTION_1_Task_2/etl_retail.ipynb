{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2188275c",
   "metadata": {},
   "source": [
    "## Section 1:\n",
    "\n",
    "### Task 2: ETL Process Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210111a7",
   "metadata": {},
   "source": [
    "**Dataset used**: https://archive.ics.uci.edu/dataset/352/online+retail\n",
    "\n",
    "**Dataset Information:**\n",
    "\n",
    "This is a transactional data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\n",
    "\n",
    "**Variables**:\n",
    "\n",
    "- `InvoiceNo` – (categorical) a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.\t\n",
    "\n",
    "- `StockCode` – (categorical) a 5-digit integral number uniquely assigned to each distinct product\n",
    "\n",
    "- `Description` – (categorical) product name\n",
    "\n",
    "- `Quantity` – (Integer) the quantities of each product (item) per transaction\n",
    "\n",
    "- `InvoiceDate` – (Date) the day and time when each transaction was generated\n",
    "\n",
    "- `UnitPrice` – (Continuous) product price per unit, in sterling\n",
    "\n",
    "- `CustomerID` – (Categorical) a 5-digit integral number uniquely assigned to each customer\n",
    "\n",
    "- `Country` – (Categorical) the name of the country where each customer resides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa12c1bf",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1d97b9",
   "metadata": {},
   "source": [
    "**Instruction:**\n",
    "\n",
    "1. **Extract:** Write Python code to read the CSV file into a pandas DataFrame. Handle any missing values or data types (e.g., convert\n",
    "InvoiceDate to datetime)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed54dffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4b32cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are the first 5 observations in the dataset: \n",
      "\n",
      "  InvoiceNo StockCode                          Description  Quantity  \\\n",
      "0    536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
      "1    536365     71053                  WHITE METAL LANTERN         6   \n",
      "2    536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
      "3    536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
      "4    536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n",
      "\n",
      "          InvoiceDate  UnitPrice  CustomerID         Country  \n",
      "0 2010-12-01 08:26:00       2.55     17850.0  United Kingdom  \n",
      "1 2010-12-01 08:26:00       3.39     17850.0  United Kingdom  \n",
      "2 2010-12-01 08:26:00       2.75     17850.0  United Kingdom  \n",
      "3 2010-12-01 08:26:00       3.39     17850.0  United Kingdom  \n",
      "4 2010-12-01 08:26:00       3.39     17850.0  United Kingdom  \n"
     ]
    }
   ],
   "source": [
    "# Reading the CSV file into a pandas DataFrame\n",
    "df = pd.read_excel(\"Online Retail.xlsx\") \n",
    "\n",
    "# Displaying the data through '.head()'\n",
    "print(f\"Below are the first 5 observations in the dataset: \\n\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ceaab17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 541909\n",
      "Number of columns: 8\n",
      "\n",
      "\n",
      "Summary of the data types and non-null counts in the dataset: \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 541909 entries, 0 to 541908\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count   Dtype         \n",
      "---  ------       --------------   -----         \n",
      " 0   InvoiceNo    541909 non-null  object        \n",
      " 1   StockCode    541909 non-null  object        \n",
      " 2   Description  540455 non-null  object        \n",
      " 3   Quantity     541909 non-null  int64         \n",
      " 4   InvoiceDate  541909 non-null  datetime64[ns]\n",
      " 5   UnitPrice    541909 non-null  float64       \n",
      " 6   CustomerID   406829 non-null  float64       \n",
      " 7   Country      541909 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(2), int64(1), object(4)\n",
      "memory usage: 33.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Displaying the number of rows and columns in 'Online Retail.xlsx'\n",
    "num_rows, num_columns = df.shape\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "print(f\"Number of columns: {num_columns}\")\n",
    "\n",
    "print(f\"\\n\")\n",
    "\n",
    "# Describing the dataset through '.info()'\n",
    "print(f\"Summary of the data types and non-null counts in the dataset: \\n\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e6468a",
   "metadata": {},
   "source": [
    "- As seen from above, this dataset has 8 columns and 541908 rows.\n",
    "\n",
    "- All the columns have the correct data types apart from `CustomerID` which is 'float64' (a numeric value) and should be converted to a string ('object').\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfca938",
   "metadata": {},
   "source": [
    "Duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "024633cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of duplicate rows include: \n",
      " 5268 \n",
      "\n",
      "The number of duplicate rows after dropping include: \n",
      " 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Duplicate rows\n",
    "\n",
    "# Checking for duplicate rows\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"The number of duplicate rows include: \\n {duplicates} \\n\")\n",
    "\n",
    "# Remove duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Checking for duplicate rows after dropping\n",
    "duplicates_after = df.duplicated().sum()\n",
    "print(f\"The number of duplicate rows after dropping include: \\n {duplicates_after} \\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6328dbc",
   "metadata": {},
   "source": [
    "Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d0c2f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of missing values in each column are: \n",
      " InvoiceNo           0\n",
      "StockCode           0\n",
      "Description      1454\n",
      "Quantity            0\n",
      "InvoiceDate         0\n",
      "UnitPrice           0\n",
      "CustomerID     135037\n",
      "Country             0\n",
      "dtype: int64 \n",
      "\n",
      "The number of missing values in each column after dropping are: \n",
      " InvoiceNo      0\n",
      "StockCode      0\n",
      "Description    0\n",
      "Quantity       0\n",
      "InvoiceDate    0\n",
      "UnitPrice      0\n",
      "CustomerID     0\n",
      "Country        0\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Missing values\n",
    "\n",
    "# Calculating the total number of missing values for each column\n",
    "missing_values = df.isnull().sum()\n",
    "print(f\"The number of missing values in each column are: \\n {missing_values} \\n\")\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(subset=['Description', 'CustomerID'], inplace=True)\n",
    "\n",
    "# Calculating the total number of missing values for each column after dropping\n",
    "missing_values_after = df.isnull().sum()\n",
    "print(f\"The number of missing values in each column after dropping are: \\n {missing_values_after} \\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202032c1",
   "metadata": {},
   "source": [
    "Data type conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d4fd7c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data types for each column are: \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 401604 entries, 0 to 541908\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count   Dtype         \n",
      "---  ------       --------------   -----         \n",
      " 0   InvoiceNo    401604 non-null  object        \n",
      " 1   StockCode    401604 non-null  object        \n",
      " 2   Description  401604 non-null  object        \n",
      " 3   Quantity     401604 non-null  int64         \n",
      " 4   InvoiceDate  401604 non-null  datetime64[ns]\n",
      " 5   UnitPrice    401604 non-null  float64       \n",
      " 6   CustomerID   401604 non-null  object        \n",
      " 7   Country      401604 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(1), int64(1), object(5)\n",
      "memory usage: 27.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Converting  'InvoiceDate' column to datetime format\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "\n",
    "# Converting  'CustomerID' column to string\n",
    "df['CustomerID'] = df['CustomerID'].astype(str)\n",
    "\n",
    "# Rechecking if the columns have the correct data types through '.info()'\n",
    "print(f\"The data types for each column are: \\n\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1760d43e",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f279955a",
   "metadata": {},
   "source": [
    "**Instruction:**\n",
    "\n",
    "2. **Transform:** \n",
    "\n",
    "- Calculate a new column: TotalSales = Quantity * UnitPrice.\n",
    "\n",
    "- Create dimension-like extracts: Group by CustomerID to create a customer\n",
    "summary (e.g., total purchases, country).\n",
    "\n",
    "- Filter data for sales in the last year (assume current date as August 12, 2025).\n",
    "\n",
    "- Handle outliers: Remove rows where Quantity < 0 or UnitPrice <= 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ba05f2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InvoiceDate starts at: 2010-12-01 08:26:00 up until 2011-12-09 12:50:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12588\\2194750558.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_recent['Date'] = df_recent['InvoiceDate'].dt.date\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12588\\2194750558.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_recent['Year'] = df_recent['InvoiceDate'].dt.year\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12588\\2194750558.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_recent['Month'] = df_recent['InvoiceDate'].dt.month\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12588\\2194750558.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_recent['Day'] = df_recent['InvoiceDate'].dt.day\n"
     ]
    }
   ],
   "source": [
    "# Removing outliers by keeping rows where the 'Quantity' is greater that 0 and 'UnitPrice' is greater than 0\n",
    "df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]\n",
    "\n",
    "\n",
    "# Creating a new column for total sales\n",
    "df['TotalSales'] = df['Quantity'] * df['UnitPrice']\n",
    "\n",
    "\n",
    "# Data is between 01/12/2010 and 09/12/2011\n",
    "# Confirm the range of the date in the data\n",
    "print('InvoiceDate starts at:', df['InvoiceDate'].min(), 'up until', df['InvoiceDate'].max())\n",
    "\n",
    "# Filter data for sales for last year's data \n",
    "cutoff_date = pd.to_datetime('2011-01-01')\n",
    "# Obtaining records above the cutoff date to get sales for the last one year\n",
    "df_recent = df[df['InvoiceDate'] > cutoff_date]\n",
    "\n",
    "\n",
    "# Creating a customer dimension table\n",
    "customer_dim = df_recent.groupby('CustomerID').agg({\n",
    "    'TotalSales': 'sum',\n",
    "    'Country': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "# Creating a Time dimension table\n",
    "# Extracting the date only from 'InvoiveDate'\n",
    "df_recent['Date'] = df_recent['InvoiceDate'].dt.date\n",
    "# Extracting the year from 'InvoiveDate'\n",
    "df_recent['Year'] = df_recent['InvoiceDate'].dt.year\n",
    "# Extracting the month from 'InvoiveDate'\n",
    "df_recent['Month'] = df_recent['InvoiceDate'].dt.month\n",
    "# Extracting the day from 'InvoiveDate'\n",
    "df_recent['Day'] = df_recent['InvoiceDate'].dt.day\n",
    "\n",
    "# Creating time_dim which stores the variables\n",
    "time_dim = df_recent[['Date', 'Year', 'Month', 'Day']].drop_duplicates()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfe3d38",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ecf4e5",
   "metadata": {},
   "source": [
    "**Instruction:**\n",
    "\n",
    "3. **Load:** Use sqlite3 in Python to create a database file (retail_dw.db). Load the transformed data into a fact table (SalesFact) and at least two dimension tables (e.g.,CustomerDim, TimeDim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "499a3dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fact Table(SalesFact) and two dimension tables (CustomerDim and TimDim) loaded.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to SQLite (it will create the file if it doesn't exist)\n",
    "conn = sqlite3.connect('retail_dw.db')\n",
    "\n",
    "# Load tables\n",
    "df_recent.to_sql('SalesFact', conn, if_exists='replace', index=False)\n",
    "customer_dim.to_sql('CustomerDim', conn, if_exists='replace', index=False)\n",
    "time_dim.to_sql('TimeDim', conn, if_exists='replace', index=False)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print('Fact Table(SalesFact) and two dimension tables (CustomerDim and TimDim) loaded.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "80e7a0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files generated for loaded data.\n"
     ]
    }
   ],
   "source": [
    "# Exporting the loaded tables to CSV files\n",
    "\n",
    "conn = sqlite3.connect('retail_dw.db')\n",
    "\n",
    "# Exporting the fact table (SalesFact)\n",
    "SalesFact_df = pd.read_sql_query('SELECT * FROM SalesFact', conn)\n",
    "SalesFact_df.to_csv('Data/SalesFact.csv', index=False)\n",
    "\n",
    "# Export dimension table (CustomerDim)\n",
    "CustomerDim_df = pd.read_sql_query('SELECT * FROM CustomerDim', conn)\n",
    "CustomerDim_df.to_csv('Data/CustomerDim.csv', index=False)\n",
    "\n",
    "# Export dimension table (TimeDim)\n",
    "TimeDim_df = pd.read_sql_query('SELECT * FROM TimeDim', conn)\n",
    "TimeDim_df.to_csv('Data/TimeDim.csv', index=False)\n",
    "\n",
    "conn.close()\n",
    "print('CSV files generated for loaded data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ce8300",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e86d59",
   "metadata": {},
   "source": [
    "**Instruction:**\n",
    "\n",
    "4. Write a function to perform the full ETL and log the number of rows processed at each stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "81795bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def run_etl():\n",
    "    logging.info(\"Starting ETL process...\")\n",
    "\n",
    "    try:\n",
    "        # Extract\n",
    "        df = pd.read_excel(\"Online Retail.xlsx\")\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        df.dropna(subset=['Description', 'CustomerID'], inplace=True)\n",
    "        df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "        df['CustomerID'] = df['CustomerID'].astype(str)\n",
    "        logging.info(f\"Extracted {len(df)} rows.\")\n",
    "\n",
    "\n",
    "        # Transform\n",
    "        df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]\n",
    "        df['TotalSales'] = df['Quantity'] * df['UnitPrice']\n",
    "        df_recent = df[df['InvoiceDate'] > pd.to_datetime('2011-01-01')]\n",
    "        logging.info(f\"{len(df_recent)} rows after filtering for last year's sales.\")\n",
    "\n",
    "        customer_dim = df_recent.groupby('CustomerID').agg({\n",
    "            'TotalSales': 'sum',\n",
    "            'Country': 'first'\n",
    "        }).reset_index()\n",
    "        logging.info(f\"{len(customer_dim)} number of customer records in dimension.\")\n",
    "\n",
    "        df_recent['Date'] = df_recent['InvoiceDate'].dt.date\n",
    "        df_recent['Year'] = df_recent['InvoiceDate'].dt.year\n",
    "        df_recent['Month'] = df_recent['InvoiceDate'].dt.month\n",
    "        df_recent['Day'] = df_recent['InvoiceDate'].dt.day\n",
    "        time_dim = df_recent[['Date', 'Year', 'Month', 'Day']].drop_duplicates()\n",
    "        logging.info(f\"{len(time_dim)} time records in dimension.\")\n",
    "\n",
    "        # Load\n",
    "        conn = sqlite3.connect('retail_dw.db')\n",
    "        df_recent.to_sql('SalesFact', conn, if_exists='replace', index=False)\n",
    "        customer_dim.to_sql('CustomerDim', conn, if_exists='replace', index=False)\n",
    "        time_dim.to_sql('TimeDim', conn, if_exists='replace', index=False)\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "        # Exporting the loaded tables to CSV\n",
    "        SalesFact_df = pd.read_sql_query('SELECT * FROM SalesFact', conn)\n",
    "        SalesFact_df.to_csv('Data/SalesFact.csv', index=False)\n",
    "    \n",
    "        CustomerDim_df = pd.read_sql_query('SELECT * FROM CustomerDim', conn)\n",
    "        CustomerDim_df.to_csv('Data/CustomerDim.csv', index=False)\n",
    "\n",
    "        TimeDim_df = pd.read_sql_query('SELECT * FROM TimeDim', conn)\n",
    "        TimeDim_df.to_csv('Data/TimeDim.csv', index=False)\n",
    "\n",
    "\n",
    "        logging.info(\"ETL process completed successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"ETL process failed: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
